
    <html>
      <head>
        <title>K Nearest Neighbors</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
      </head>
      <body>
        <div id='content'>
    <h1 id="k-nearest-neighbors-project">K Nearest Neighbors Project</h1>
<p>Welcome to the KNN Project! This will be a simple project implementing the K Nearest Neighbors Algorithm. Go ahead and just follow the directions below.</p>
<h2 id="import-libraries">Import Libraries</h2>
<p><strong>Import pandas,seaborn, and the usual libraries.</strong></p>
<pre><code class="python language-python">import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
</code></pre>
<h2 id="get-the-data">Get the Data</h2>
<p>** Read the 'KNN<em>Project</em>Data csv file into a dataframe **</p>
<pre><code class="python language-python">df = pd.read_csv('KNN_Project_Data')
</code></pre>
<p><strong>Check the head of the dataframe.</strong></p>
<pre><code class="python language-python">df.head()
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>XVPM</th>
      <th>GWYH</th>
      <th>TRAT</th>
      <th>TLLZ</th>
      <th>IGGA</th>
      <th>HYKR</th>
      <th>EDFS</th>
      <th>GUUB</th>
      <th>MGJM</th>
      <th>JHZC</th>
      <th>TARGET CLASS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1636.670614</td>
      <td>817.988525</td>
      <td>2565.995189</td>
      <td>358.347163</td>
      <td>550.417491</td>
      <td>1618.870897</td>
      <td>2147.641254</td>
      <td>330.727893</td>
      <td>1494.878631</td>
      <td>845.136088</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1013.402760</td>
      <td>577.587332</td>
      <td>2644.141273</td>
      <td>280.428203</td>
      <td>1161.873391</td>
      <td>2084.107872</td>
      <td>853.404981</td>
      <td>447.157619</td>
      <td>1193.032521</td>
      <td>861.081809</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1300.035501</td>
      <td>820.518697</td>
      <td>2025.854469</td>
      <td>525.562292</td>
      <td>922.206261</td>
      <td>2552.355407</td>
      <td>818.676686</td>
      <td>845.491492</td>
      <td>1968.367513</td>
      <td>1647.186291</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1059.347542</td>
      <td>1066.866418</td>
      <td>612.000041</td>
      <td>480.827789</td>
      <td>419.467495</td>
      <td>685.666983</td>
      <td>852.867810</td>
      <td>341.664784</td>
      <td>1154.391368</td>
      <td>1450.935357</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1018.340526</td>
      <td>1313.679056</td>
      <td>950.622661</td>
      <td>724.742174</td>
      <td>843.065903</td>
      <td>1370.554164</td>
      <td>905.469453</td>
      <td>658.118202</td>
      <td>539.459350</td>
      <td>1899.850792</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
<h1 id="eda">EDA</h1>
<p>Since this data is artificial, we'll just do a large pairplot with seaborn.</p>
<p><strong>Use seaborn on the dataframe to create a pairplot with the hue indicated by the TARGET CLASS column.</strong></p>
<pre><code class="python language-python">sns.pairplot(df, hue='TARGET CLASS')
</code></pre>
<pre><code>&lt;seaborn.axisgrid.PairGrid at 0x7fa9b6f33b90&gt;
</code></pre>
<p><img src="K_Nearest_Neighbors_files/K_Nearest_Neighbors_7_1.png" alt="png" /></p>
<h1 id="standardize-the-variables">Standardize the Variables</h1>
<p>Time to standardize the variables.</p>
<p>** Import StandardScaler from Scikit learn.**</p>
<pre><code class="python language-python">from sklearn.preprocessing import StandardScaler
</code></pre>
<p>** Create a StandardScaler() object called scaler.**</p>
<pre><code class="python language-python">scaler = StandardScaler()
</code></pre>
<p>** Fit scaler to the features.**</p>
<pre><code class="python language-python">scaler.fit(df.drop('TARGET CLASS', axis=1))
</code></pre>
<pre><code>StandardScaler(copy=True, with_mean=True, with_std=True)
</code></pre>
<p><strong>Use the .transform() method to transform the features to a scaled version.</strong></p>
<pre><code class="python language-python">scaled_features = scaler.transform(df.drop('TARGET CLASS', axis=1))
</code></pre>
<p><strong>Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.</strong></p>
<pre><code class="python language-python">df_features = pd.DataFrame(data=scaled_features, columns=df.columns[:-1])
df_features.head()
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>XVPM</th>
      <th>GWYH</th>
      <th>TRAT</th>
      <th>TLLZ</th>
      <th>IGGA</th>
      <th>HYKR</th>
      <th>EDFS</th>
      <th>GUUB</th>
      <th>MGJM</th>
      <th>JHZC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.568522</td>
      <td>-0.443435</td>
      <td>1.619808</td>
      <td>-0.958255</td>
      <td>-1.128481</td>
      <td>0.138336</td>
      <td>0.980493</td>
      <td>-0.932794</td>
      <td>1.008313</td>
      <td>-1.069627</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.112376</td>
      <td>-1.056574</td>
      <td>1.741918</td>
      <td>-1.504220</td>
      <td>0.640009</td>
      <td>1.081552</td>
      <td>-1.182663</td>
      <td>-0.461864</td>
      <td>0.258321</td>
      <td>-1.041546</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.660647</td>
      <td>-0.436981</td>
      <td>0.775793</td>
      <td>0.213394</td>
      <td>-0.053171</td>
      <td>2.030872</td>
      <td>-1.240707</td>
      <td>1.149298</td>
      <td>2.184784</td>
      <td>0.342811</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.011533</td>
      <td>0.191324</td>
      <td>-1.433473</td>
      <td>-0.100053</td>
      <td>-1.507223</td>
      <td>-1.753632</td>
      <td>-1.183561</td>
      <td>-0.888557</td>
      <td>0.162310</td>
      <td>-0.002793</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.099059</td>
      <td>0.820815</td>
      <td>-0.904346</td>
      <td>1.609015</td>
      <td>-0.282065</td>
      <td>-0.365099</td>
      <td>-1.095644</td>
      <td>0.391419</td>
      <td>-1.365603</td>
      <td>0.787762</td>
    </tr>
  </tbody>
</table>
</div>
<h1 id="train-test-split">Train Test Split</h1>
<p><strong>Use train<em>test</em>split to split your data into a training set and a testing set.</strong></p>
<pre><code class="python language-python">X = df_features
y = df['TARGET CLASS']
</code></pre>
<pre><code class="python language-python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)
</code></pre>
<h1 id="using-knn">Using KNN</h1>
<p><strong>Import KNeighborsClassifier from scikit learn.</strong></p>
<pre><code class="python language-python">from sklearn.neighbors import KNeighborsClassifier
</code></pre>
<p><strong>Create a KNN model instance with n_neighbors=1</strong></p>
<pre><code class="python language-python">knn = KNeighborsClassifier(n_neighbors=1)
</code></pre>
<p><strong>Fit this KNN model to the training data.</strong></p>
<pre><code class="python language-python">knn.fit(X_train, y_train)
</code></pre>
<pre><code>KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,
                     weights='uniform')
</code></pre>
<h1 id="predictions-and-evaluations">Predictions and Evaluations</h1>
<p>Let's evaluate our KNN model!</p>
<p><strong>Use the predict method to predict values using your KNN model and X_test.</strong></p>
<pre><code class="python language-python">predictions = knn.predict(X_test)
</code></pre>
<p>** Create a confusion matrix and classification report.**</p>
<pre><code class="python language-python">from sklearn.metrics import classification_report, confusion_matrix
</code></pre>
<pre><code class="python language-python">print(confusion_matrix(y_test, predictions))
</code></pre>
<pre><code>[[109  43]
 [ 41 107]]
</code></pre>
<pre><code class="python language-python">print(classification_report(y_test, predictions))
</code></pre>
<pre><code>              precision    recall  f1-score   support

           0       0.73      0.72      0.72       152
           1       0.71      0.72      0.72       148

    accuracy                           0.72       300
   macro avg       0.72      0.72      0.72       300
weighted avg       0.72      0.72      0.72       300
</code></pre>
<h1 id="choosing-a-k-value">Choosing a K Value</h1>
<p>Let's go ahead and use the elbow method to pick a good K Value!</p>
<p>** Create a for loop that trains various KNN models with different k values, then keep track of the error_rate for each of these models with a list. Refer to the lecture if you are confused on this step.**</p>
<pre><code class="python language-python">error_rate = []

for i in range(1, 50):
  knn = KNeighborsClassifier(n_neighbors=i)
  knn.fit(X_train, y_train)
  predict_i = knn.predict(X_test)
  error_rate.append(np.mean(predict_i != y_test))
</code></pre>
<p><strong>Now create the following plot using the information from your for loop.</strong></p>
<pre><code class="python language-python">plt.figure(figsize=(10,6))
plt.plot(range(1,50), error_rate, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)
plt.title('Error Rate Vs K Value')
plt.xlabel('K Value')
plt.ylabel('Error Rate')
</code></pre>
<pre><code>Text(0, 0.5, 'Error Rate')
</code></pre>
<p><img src="K_Nearest_Neighbors_files/K_Nearest_Neighbors_37_1.png" alt="png" /></p>
<h2 id="retrain-with-new-k-value">Retrain with new K Value</h2>
<p><strong>Retrain your model with the best K value (up to you to decide what you want) and re-do the classification report and the confusion matrix.</strong></p>
<pre><code class="python language-python">knn = KNeighborsClassifier(n_neighbors=38) #K = 38
knn.fit(X_train, y_train)
predictions = knn.predict(X_test)

print(confusion_matrix(y_test, predictions))
print(classification_report(y_test, predictions))
</code></pre>
<pre><code>[[126  26]
 [ 23 125]]
              precision    recall  f1-score   support

           0       0.85      0.83      0.84       152
           1       0.83      0.84      0.84       148

    accuracy                           0.84       300
   macro avg       0.84      0.84      0.84       300
weighted avg       0.84      0.84      0.84       300
</code></pre>
<h1 id="great-job">Great Job!</h1>

        </div>
        <style type='text/css'>body {
  font: 400 16px/1.5 "Helvetica Neue", Helvetica, Arial, sans-serif;
  color: #111;
  background-color: #fdfdfd;
  -webkit-text-size-adjust: 100%;
  -webkit-font-feature-settings: "kern" 1;
  -moz-font-feature-settings: "kern" 1;
  -o-font-feature-settings: "kern" 1;
  font-feature-settings: "kern" 1;
  font-kerning: normal;
  padding: 30px;
}

@media only screen and (max-width: 600px) {
  body {
    padding: 5px;
  }

  body > #content {
    padding: 0px 20px 20px 20px !important;
  }
}

body > #content {
  margin: 0px;
  max-width: 900px;
  border: 1px solid #e1e4e8;
  padding: 10px 40px;
  padding-bottom: 20px;
  border-radius: 2px;
  margin-left: auto;
  margin-right: auto;
}

hr {
  color: #bbb;
  background-color: #bbb;
  height: 1px;
  flex: 0 1 auto;
  margin: 1em 0;
  padding: 0;
  border: none;
}

/**
 * Links
 */
a {
  color: #0366d6;
  text-decoration: none; }
  a:visited {
    color: #0366d6; }
  a:hover {
    color: #0366d6;
    text-decoration: underline; }

pre {
  background-color: #f6f8fa;
  border-radius: 3px;
  font-size: 85%;
  line-height: 1.45;
  overflow: auto;
  padding: 16px;
}

/**
  * Code blocks
  */

code {
  background-color: rgba(27,31,35,.05);
  border-radius: 3px;
  font-size: 85%;
  margin: 0;
  word-wrap: break-word;
  padding: .2em .4em;
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
}

pre > code {
  background-color: transparent;
  border: 0;
  display: inline;
  line-height: inherit;
  margin: 0;
  overflow: visible;
  padding: 0;
  word-wrap: normal;
  font-size: 100%;
}


/**
 * Blockquotes
 */
blockquote {
  margin-left: 30px;
  margin-top: 0px;
  margin-bottom: 16px;
  border-left-width: 3px;
  padding: 0 1em;
  color: #828282;
  border-left: 4px solid #e8e8e8;
  padding-left: 15px;
  font-size: 18px;
  letter-spacing: -1px;
  font-style: italic;
}
blockquote * {
  font-style: normal !important;
  letter-spacing: 0;
  color: #6a737d !important;
}

/**
 * Tables
 */
table {
  border-spacing: 2px;
  display: block;
  font-size: 14px;
  overflow: auto;
  width: 100%;
  margin-bottom: 16px;
  border-spacing: 0;
  border-collapse: collapse;
}

td {
  padding: 6px 13px;
  border: 1px solid #dfe2e5;
}

th {
  font-weight: 600;
  padding: 6px 13px;
  border: 1px solid #dfe2e5;
}

tr {
  background-color: #fff;
  border-top: 1px solid #c6cbd1;
}

table tr:nth-child(2n) {
  background-color: #f6f8fa;
}

/**
 * Others
 */

img {
  max-width: 100%;
}

p {
  line-height: 24px;
  font-weight: 400;
  font-size: 16px;
  color: #24292e; }

ul {
  margin-top: 0; }

li {
  color: #24292e;
  font-size: 16px;
  font-weight: 400;
  line-height: 1.5; }

li + li {
  margin-top: 0.25em; }

* {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
  color: #24292e; }

a:visited {
  color: #0366d6; }

h1, h2, h3 {
  border-bottom: 1px solid #eaecef;
  color: #111;
  /* Darker */ }</style>
      </body>
    </html>